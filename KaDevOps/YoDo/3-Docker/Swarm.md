# Сборка кластера Docker Swarm
Для прохождения практики тебе потребуется 2-3 машины с linux и docker на борту.
Сначала инициализируем кластер на первой мастер-ноде. Она будет главной.
А потом добавляем остальные ноды.

## Инцициализация кластера делается вот так:
`sudo docker swarm init --advertise-addr 172.21.0.14`

Как видишь, при создании кластера/роя выводится подсказка с командой. Ее надо вводить на остальных нодах, чтобы добавить их в рой/кластер.

Если ты случайно закрыл окно и забыл что там было написано, то получить это сообщение можно с помощью команды:
`sudo docker swarm join-token manager`

Заходи на вторую ноду и подключай к рою.
`sudo docker swarm join --token SWMTKN-1-68ya9g3ag5fjv8iq7sdq0vw8rjs3gp9fbmhkixcuon4lfji3r4-en2gdkxaox2i2nph24nyx2r32 192.168.88.63:2377`
У меня получилось 
172.21.0.14 мастер
172.21.0.151 Ansible
172.21.1.188 test Centos

Посмотреть состояние кластера и список подключенных нод можно командой:
`sudo docker node ls`
Docker Swarm для своей работы использует следующие порты:
Эти порты должны быть доступны от каждой ноды к любой другой.
2377/tcp
7946/tcp
7946/udp
4889/udp

Мастер-нода по умолчанию не отлынивает от работы и выполняет все те же задачи, что и подчиненные ноды.
Но можно принудительно вывести ее в чистые начальники. Нода будет заниматься управлением, но ни один контейнер на ней выполняться не будет.

Делается это так:
`docker node update --availability drain name_master`

Чтобы опять сделать мастер-ноду работягой можно выполнить вот такую команду:
`sudo docker node update --availability active name_master`
Если нода хочет выйти из кластера, то на ней надо выполнить команду:
`sudo docker swarm leave`

# Сеть

Теперь пару слов о сети. Ты же помнишь, что для контейнеров на одной машине сеть делается с помощью bridge?
Так вот. Для кластеров придуман свой тип сети - overlay.
При инициализации кластера такая сеть создается автоматически. И добавляется на каждую ноду, при ее подключении к кластеру.

`docker network ls`

Вот теперь все готово для запуска первого контейнера в кластере - т.е. сервиса.
`sudo docker service create --replicas 1 --name web nginx`

Что делает эта команда?
Естественно создает сервис =). Со следующими параметрами:
- количество реплик (т.е. на каком количестве нод будет запущен сервис) = 1
- имя сервиса = web
- образ из которого будет собран сервис = nginx

А чтобы посмотреть список запущенных сервисов, надо ввести вот такую команду:
`sudo docker service ls`
Вот запустили мы с тобой сервис на одну реплику. А что если теперь хочется его расширить на две реплики, но так, чтобы не выключать?

`sudo docker service scale web=2`
Проверим контейнеры
`sudo docker ps`

В случае, если количество реплик равно количеству нод и одна из нод пропала из сети, то на одной из нод запустится второй экземпляр контейнера.
При включении ноды он обратно уползет на нее.

Сразу предупреждаю. Реплика не в прямом смысле слова переползает. Контейнер тушится на одной ноде и запускается на другой. Несохраненные данные при этом будут потеряны.
Это надо понимать при разработке контейнера, который будет работать в кластере.


Кстати, чтобы не лазать по всем нодам, чтобы посмотреть сколько и чего там запущено, можно ввести вот такую команду:
`sudo docker service ps web`

А теперь удалим сервис. Тем более он ничего полезного и не делает, да и попасть на контейнеры проблематично =).

Делается это вот так:
`sudo docker service rm web`

А теперь соберем сервис заново, но с другими параметрами. Как минимум неплохо бы 80-й порт вытащить наружу ;)
`sudo docker service create --mode=global --publish 8011:80 --name web nginx`
--mode=global - этим параметром я сказал рою, что контейнер должен быть запущен на каждой ноде. По умолчанию сервис создается в режиме replicated

Попробуй теперь уменьшить количество реплик =).
`sudo docker service scale web=1`
Не вышло? Ну так и должно быть. В этом режиме сервис заполоняет все доступные ноды. 
Придется снова разобрать и собрать сервис.
`sudo docker service rm web`
`sudo docker service create --replicas 1 --publish 8011:80 --name web nginx`

Ну и попробуй открыть веб-сайт на первой и второй нодах.

Итак, надо обновить ПО так, чтобы никто не заметил простоя.
Так вот. Мы выполняем команду rollup-update для сервисов.:
`docker service update --update-parallelism 1 --update-delay 5s --image webapp:2.0 web`
И каждая реплика обновляется по очереди с интервалом в 5 секунд, в то время как другие реплики остаются доступными.
Никакого простоя. Ни один пользователь ничего не заметит.
а что будет, если единственная управлющая нода сдохнет? =).
Без рулевого коллектив пойдет вразнос и будет простой сервиса.
А вот чтобы этого избежать нужны резервные менеджеры. Да хоть все ноды можно объявить менеджерами. Тогда при кончине одной из управляющих нод, один из резервных менеджеров возьмет управление на себя.

Можно повышать воркеров до менеджеров:
`docker node promote docker-node1`

Проверим статус
`sudo docker node ls`

Но можно ноду сразу подключать в рой в качестве менеджера.
Делается это так:

На мастер-ноде запрашивается отдельный токен для подключения менеджеров:
`sudo docker swarm join-token manager`

А после этого показанная команда используется для подключения новых нод.

А если надо менеджера разжаловать до воркера, то делается это вот так:
`sudo docker demote docker-node1`

# Теперь поговорим о стратегиях размещения реплик на нодах.

В общем и целом у Docker Swarm есть две стратегии:
1.Доступность ресурсов – планировщик Docker Swarm имеет информацию о доступных ресурсах на рабочих нодах и запускает задачу на наименее загруженной рабочей ноде
2.Метки(labels) и ограничение(constrains)

Один сервер может быть добавлен только в один рой. А иногда требуется внутри роя как-то разграничить ресурсы самостоятельно. Для этого и применяются метки.
Метки делятся на стандартные, автоматические и заданные пользователем.
Стандартные метки есть всегда (к примеру node.hostname). В терминологии swarm это node.labels. Эти метки либо задаются докером, либо собираются с конкретного контейнера.
Но когда их недостаточно, то можно задать свои - engine.labels.

Чтобы воспользоваться механизмом меток, нужно при создании сервиса указать требуемую метку для размещения приложения.

Создадим еще один сервис - web2 с образом nginx, но жить он должен всегда на ноде test

`sudo docker service create --constraint 'node.hostname == test' --name web2 --replicas 1 --publish 8012:80 nginx`

А как сделать свою метку?
Вот примерно вот так:
`sudo docker node update --label-add Name_Label Name_Nod`

А просмотреть метки можно вот так:
`sudo docker node inspect --pretty docker-node1`
или 
`sudo docker node inspect --pretty z800`

И теперь можно запускать приложение, точно зная, что оно запустится на ноде с меткой.
`sudo docker service create --constraint 'node.labels.ROOM == metka-1' --name TestLabels --replicas 1 --publish 8889:8080 tomcat:latest`

Это все, что я хотел тебе показать из возможностей docker swarm при управлении в ручном режиме.
С этими знаниями тебе будет проще отлаживать ошибки своих кластерных конфигураций.
