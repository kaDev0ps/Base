# [Домашнее задание](https://github.com/a-prokopyev-resume/virt-homeworks/tree/virt-11/05-virt-01-basics) к [занятию 1.  «Введение в виртуализацию. Типы и функции гипервизоров. Обзор рынка вендоров и областей применения»](https://netology.ru/profile/program/virtd-27/lessons/274655/lesson_items/1471779)

## Задача 1

1. Аппаратная виртуализация не требует модификации ядер гостевых OS, поэтому обычно может запускать почти любые гостевые OS, которые не знают о том, что запускаются на виртуальном оборудовании.   
Примеры гипервизоров: KVM, XEN, bhyve, ESXi, Hyper-V.

2. Паравиртуализация использует поддержку гостевых ядер (модифицированных и со специальными драйверами типа virtio в случае KVM) для более оптимального выполнения части операций, обеспечивает максимальную производительность по сравнению с другими типами полной (с гостевым ядром) виртуализации. Недостаток - ограниченная поддержка гостевых OS.   
Примеры гипервизоров: KVM, XEN(может работать даже без аппаратного ускорения VT-D like), bhyve, Hyper-V (только для некоторых видов гостевых Linux).

3. Виртуализация на основе OS использует для контейнеров общее хостовое ядро, при этом изолирует контейнеры с помощью таких подсистем хостового ядра как namespaces и ограничивает потребляемые ими ресурсы с помощью cgroups. Максимальная производительность, даже быстрее паравиртуализации.  
Примеры реализаций: LXC, Docker, OpenVZ, Virtuozzo, Solaris zones, FreeBSD jail.

Полезные линки:  
[Таблица сравнения гипервизоров](https://ru.wikipedia.org/wiki/%D0%A1%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD)  
[Еще одна сравнительная таблица гипервизоров](http://citforum.ru/operating_systems/virtualization/t4.shtml)  
[Анализ технологий виртуализации](https://habr.com/ru/companies/southbridge/articles/212985/)  
[Подробное объяснение от IBM "Паравиртуализация vs полная виртуализация"](https://developer.ibm.com/articles/l-virtio/)   
[Объяснение "Паравиртуализация vs полная виртуализация" простыми словами](https://qna.habr.com/q/500711)  
[Рыбаков С.А. - Методы виртуализации подсистемы ввода-вывода](http://www.mcst.ru/files/5e6f85/040cd8/501d78/000000/s._a._rybakov_metody_virtualizatsii_podsistemy_vvoda-vyvoda.pdf)   
[Wikipedia - Paravirtualization](https://en.wikipedia.org/wiki/Paravirtualization) 

## Задача 2

1. Высоконагруженная база данных, чувствительная к отказу.

Если это классическая реляционная ACID СУБД, для которой ненужно много нод для обеспечения HA, обычно 2-3 ноды, 
то можно воспользоваться локальными физическими машинами без виртуализации либо паравиртуальными на разных физических хостах (например, при хостинге в  IaaS).
IMHO виртуализация может помочь быстрее вводить в строй дополнительные запасные реплики СУБД в случае сбоя оборудования на части реплик кластера СУБД. 

2. Различные web-приложения.

Для веб приложений хорошо подходят контейнеры (виртуализация уровня ОС) с внешней СУБД (из предыдущего пункта). Некоторые виды приложений бывают и вовсе stateless.
В тоже время в качестве нод кластера для запуска контейнеров удобно использовать хосты с паравиртуализацией.
При этом желательно запускать разные реплики одного и того же сервиса на нодах HA кластера, расположенных на разных физических хостах,
чтобы при сбое одного из физических хостов виртуальные ноды и контейнеры в них продолжали свою работу хотя бы в части реплик.

3. Windows-системы для использования бухгалтерским отделом.

Windows всегда желательно запускать в виртуальных машинах со снэпшотами системного раздела, потому что эта система иногда ведет 
себя непредсказуемо, и бывает намного проще и быстрее откатить системный раздел к последнему снэпшоту, чем пытаться отремонтировать настройки Windows системы и приложений в случае их падения.  

4. Системы, выполняющие высокопроизводительные расчёты на GPU.

Таким приложениям необходим доступ к оборудованию ускорителя через драйвера ядра, поэтому их можно запускать либо на физическом хосте
либо на виртуальной машине с пробросом PCI интерфейса GPU в виртуалку. Но судя по https://bobcares.com/blog/proxmox-gpu-passthrough-lxc/
доступ к GPU возможен даже из контейнеров. Если вычислительная нагрузка на CPU невелика и основную часть работы выполняет GPU, 
то выбирать способ развертывания таких приложений нужно исходя из удобства администрирования и использования. 


## Задача 3

1. 100 виртуальных машин на базе Linux и Windows, общие задачи, нет особых требований. 
    Преимущественно Windows based-инфраструктура, требуется реализация программных балансировщиков нагрузки, 
    репликации данных и автоматизированного механизма создания резервных копий.

Hyper-V или VMWare vSphere. Но я бы предпочел KVM под управлением OpenNebula или Proxmox, потому что они open-source. 

2. Требуется наиболее производительное бесплатное open source-решение для виртуализации небольшой (20-30 серверов) инфраструктуры на базе Linux и Windows виртуальных машин.

XEN или KVM в режиме паравиртуализации.

3. Необходимо бесплатное, максимально совместимое и производительное решение для виртуализации Windows-инфраструктуры.

Вероятно Microsoft Hyper-V Server, но я бы предпочел Proxmox или OpenNebula :)

4. Необходимо рабочее окружение для тестирования программного продукта на нескольких дистрибутивах Linux.

Если приложение не работает напрямую с оборудованием, то можно попробовать контейнеры хотя бы LXC или даже Docker 
(если программному продукту достаточно работать в Docker контейнере).
Иначе можно использовать KVM с одним из оркестраторов типа OpenNebula.


## Задача 4 (возможные проблемы и недостатки гетерогенной среды виртуализации)

С моей точки зрения, использование гетерогенных систем в т.ч. систем виртуализации требует дополнительных затрат времени на изучение и поддержку нескольких видов систем одновременно.
Это приводит либо к увеличению количества узко специализированного обслуживающего персонала (а ведь нужно еще и дублировать роли) либо к значительному повышению нагрузки на каждого отдельно взятого специалиста,
что повышает его стоимость на рынке труда при попытке замены на другого. Например, в случае с VMWare ESXi управление его через командную строку нетривиально, потому что команды имеют довольно причудливый синтаксис и существует
несколько способов решения одной и той же задачи разными версиями CLI интерфейсов. Больше затрат времени на обновление подобных хостов виртуализации, 
меньшая предсказуемость при обновлениях, особенно проприетарных систем. Снизить издержки на поддержку гетерогенной среды виртуализации 
можно с помощью удобного оркестратора типа OpenNebula, который умеет работать с облаком, состоящим из разных гипервизоров.
Использование максимально автоматизированных методик DevOps таких, как IaC и CI/CD так же может снизить нагрузку на администраторов. 

Почти всегда по возможности нужно стремиться уходить от проприетарного софта в сторону сравнимых по функционалу open-source решений.
Относительно несложным и достаточно популярным оркестратором гипервизора KVM является открытый оркестратор Proxmox, для него проще найти специалистов на рынке труда.
Еще более интересным является оркестратор OpenNebula, потому что он поддерживает бОльшее количество различных гипервизоров (в т.ч. KVM и VMWare vSphere) и 
даже самостоятельное создание плагинов для новых гипервизоров. Кроме того OpenNebula с одной стороны можно сравнивать с решениями уровня OpenStack,
а с другой стороны он требует намного меньших административных затрат на его поддержку, в то время как для поддержки OpenStack нередко нужен целый коллектив администраторов.  
Было бы очень хорошо, если бы для OpenNebula появились плагины для управления гипервизорами bhyve на FreeBSD и Illumos системах (SmartOS, OmniOS),
а так же vmm (на OpenBSD).

Лично я иногда упоминяю свои навыки работы с проприетарными гипервизорами (ESXi и Hyper-V) только в контексте миграции с них на открытые решения типа Proxmox и OpenNebula.
Работать с проприетарными гипервизорами мне было неинтересно и убыточно с точки зрения излишних затрат моего времени (пустая трата) на обслуживание и перспектив саморазвития.
Впрочем это относится и к большей части остального софта, предпочитаю использовать только open-source и работать только с компаниями, которые 
стараются минимизировать у себя количество проприетарного софта.

Дополнительные рекомендации по выбору инфраструктуры и общеупотребительного софта для деплоя (мои наработки в качестве проверяющего эксперта - ревьювера на курсе B2G для Linux администраторов): 
https://raw.githubusercontent.com/a-prokopyev-resume/documents/main/DevOps_public.txt